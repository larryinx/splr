# SPLR Architecture Configuration

name: splr_trm_qwen_zl

# Model architecture
hidden_size: 1024
max_position_embeddings: 320
num_attention_heads: 8
num_key_value_heads: 8
intermediate_size: 2048
tie_word_embeddings: false

# TRM recursive reasoning
hierarchical_reasoning: false
L_cycles: 6
H_cycles: 3
L_layers: 2
H_layers: 0
L_grad_cycles: 6
H_grad_cycles: 1

# Halting
halt_max_steps: 16
halt_exploration_prob: 0.1

# Multi-step reasoning
max_reasoning_steps: 2

# Task embeddings
task_emb_len: 1
task_emb_ndim: 1024
num_task_identifiers: 1

# Inter-step reasoning
enable_inter_latent: true
inter_reasoning_only: true

# Embedding loading
load_embedding: Qwen/Qwen3-0.6B

# Attention
attention_bias: false
attention_dropout: 0.0

# Position encoding
rope_theta: 10000.0

# Normalization
rms_norm_eps: 1.0e-6

# Forward dtype
forward_dtype: bfloat16

# ACT options
no_ACT_continue: true
